{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"You are the worst person I know\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stygianphantom/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/stygianphantom/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# CyberBullyingClassifier\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "model_path = \"./trained_bert_cyberbullying_mendeley\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "trainer = Trainer(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Cyberbullying (Confidence: 99.72%)\n",
      "\n",
      "Token Attributions (Layer Integrated Gradients):\n",
      "[CLS]: 0.5309\n",
      "you: 0.4722\n",
      "are: 0.3987\n",
      "the: 1.6299\n",
      "worst: 2.6743\n",
      "person: 0.6498\n",
      "i: 0.8518\n",
      "know: 0.5079\n",
      "[SEP]: 0.4447\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from captum.attr import LayerIntegratedGradients\n",
    "\n",
    "# Ensure the model is on CPU\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Tokenize and prepare input\n",
    "inputs = tokenizer(test_sentence, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get logits and predicted class\n",
    "logits = outputs.logits\n",
    "predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "# Get prediction confidence\n",
    "probs = F.softmax(logits, dim=1)\n",
    "confidence = probs[0, predicted_class].item()\n",
    "\n",
    "# Map predicted class to label\n",
    "label_map = {1: \"Not Cyberbullying\", 0: \"Cyberbullying\"}\n",
    "result = label_map[predicted_class]\n",
    "\n",
    "print(f\"Prediction: {result} (Confidence: {confidence:.2%})\")\n",
    "\n",
    "# Define a forward function for IG\n",
    "def forward_func(input_ids, attention_mask):\n",
    "    output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    return output.logits[:, predicted_class]\n",
    "\n",
    "# Get embedding layer\n",
    "embedding_layer = model.get_input_embeddings()\n",
    "\n",
    "# Setup Layer Integrated Gradients\n",
    "lig = LayerIntegratedGradients(forward_func, embedding_layer)\n",
    "\n",
    "# Create baseline (PAD tokens)\n",
    "baseline_input_ids = torch.full_like(inputs[\"input_ids\"], tokenizer.pad_token_id).to(device)\n",
    "\n",
    "# Compute attributions\n",
    "attributions, delta = lig.attribute(\n",
    "    inputs=(inputs[\"input_ids\"], inputs[\"attention_mask\"]),\n",
    "    baselines=(baseline_input_ids, inputs[\"attention_mask\"]),\n",
    "    return_convergence_delta=True\n",
    ")\n",
    "\n",
    "# Sum attributions across embeddings\n",
    "attributions_sum = attributions.sum(dim=-1).squeeze(0)\n",
    "\n",
    "# Decode tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n",
    "\n",
    "# Display token-level attributions\n",
    "print(\"\\nToken Attributions (Layer Integrated Gradients):\")\n",
    "for token, score in zip(tokens, attributions_sum.detach().cpu().numpy()):\n",
    "    print(f\"{token}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hatespeech classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1\n",
      "Probability: 0.6927011\n",
      "\n",
      "Attention weights from [CLS] to each token:\n",
      "[CLS]: 0.0000\n",
      "[UNK]: 0.0026\n",
      "are: 0.0000\n",
      "the: 0.0545\n",
      "worst: 0.1406\n",
      "person: 0.0195\n",
      "[UNK]: 0.0032\n",
      "know: 0.0000\n",
      "[SEP]: 0.7795\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Step 1: Load model and tokenizer\n",
    "hate_model_path = \"hate_speech_recall_optimized_model\"\n",
    "\n",
    "hate_tokenizer = AutoTokenizer.from_pretrained(hate_model_path)\n",
    "hate_model = AutoModelForSequenceClassification.from_pretrained(hate_model_path, output_attentions=True)\n",
    "hate_model.eval()\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hate_model.to(device)\n",
    "\n",
    "# === Step 2: Input sentence ===\n",
    "inputs = hate_tokenizer(test_sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# === Step 3: Forward pass ===\n",
    "with torch.no_grad():\n",
    "    outputs = hate_model(**inputs)\n",
    "    logits = outputs.logits.cpu().numpy()\n",
    "    attentions = outputs.attentions  # shape: (num_layers, batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "# === Step 4: Apply sigmoid and threshold ===\n",
    "hate_probs = 1 / (1 + np.exp(-logits))  # Sigmoid\n",
    "preds = (hate_probs > 0.51).astype(int).flatten()\n",
    "\n",
    "# === Step 5: Optional - If true label is known, compute metrics ===\n",
    "# Dummy label for demo (replace with real one if available)\n",
    "labels = np.array([1])\n",
    "print(\"Prediction:\", preds[0])\n",
    "print(\"Probability:\", hate_probs[0][0])\n",
    "\n",
    "\n",
    "# === Step 6: Attention weights from [CLS] token ===\n",
    "last_layer_attention = attentions[-1]  # last layer: (1, num_heads, seq_len, seq_len)\n",
    "attention_matrix = last_layer_attention[0][0]  # head 0 → shape: (seq_len, seq_len)\n",
    "cls_attention = attention_matrix[0]  # [CLS] token attends to others → shape: (seq_len,)\n",
    "\n",
    "# Decode tokens\n",
    "tokens = hate_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "# Show token attention weights\n",
    "print(\"\\nAttention weights from [CLS] to each token:\")\n",
    "for token, weight in zip(tokens, cls_attention):\n",
    "    print(f\"{token}: {weight.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence target\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Set of pronouns we consider as PERSONs\n",
    "PERSON_PRONOUNS = {\"you\", \"he\", \"she\", \"they\", \"him\", \"her\"}\n",
    "\n",
    "def detect_person_targets(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    person_tokens = []\n",
    "\n",
    "    for token in doc:\n",
    "        # Add pronouns like \"you\", \"he\", etc.\n",
    "        if token.text.lower() in PERSON_PRONOUNS:\n",
    "            person_tokens.append(token.text)\n",
    "    \n",
    "    # Add spaCy named PERSON entities\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            person_tokens.append(ent.text)\n",
    "\n",
    "    # Remove duplicates and preserve order\n",
    "    seen = set()\n",
    "    unique_persons = [p for p in person_tokens if not (p.lower() in seen or seen.add(p.lower()))]\n",
    "\n",
    "    return {\n",
    "        \"person_tokens\": unique_persons,\n",
    "        \"is_directed_towards_someone\": len(unique_persons) > 0\n",
    "    }\n",
    "\n",
    "ner_person = detect_person_targets(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9071002423763275\n"
     ]
    }
   ],
   "source": [
    "stage_1_prob = 0.7*confidence + 0.3*int(ner_person['is_directed_towards_someone'])*hate_probs[0][0]\n",
    "\n",
    "print(stage_1_prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
